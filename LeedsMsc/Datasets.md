### **Academic/Open-Source Datasets of Real-World Robot Demonstrations**

#### 1. **Open X-Embodiment (OXE) Dataset**

- **By:** Google + 33 academic labs (including CMU, Stanford, MIT, Berkeley)
    
- **Size:** ~500k episodes, ~1500 tasks, 22 robot embodiments
    
- **Link:** https://openx-embodiment.github.io
    
- **Note:** Used to train RT-X and other VLA models.
    

#### 2. **RT-1 / RT-2 Datasets**

- **By:** Google Robotics
    
- **Focus:** Real robot demonstrations using mobile manipulators
    
- **Access:** RT-1 data/code is open-source: https://robotics-transformer.github.io
    

#### 3. **EGTEA Gaze+**

- **By:** University of Georgia
    
- **Focus:** Egocentric videos with gaze tracking for kitchen tasks
    
- **Link:** https://cbs.ic.gatech.edu/egtea_gaze
    

#### 4. **HoloAssist**

- **By:** Microsoft Research + CMU
    
- **Focus:** Egocentric human-AI interaction in real homes
    
- **Link:** https://holoassist.github.io
    

#### 5. **MECCANO Dataset**

- **By:** University of Catania
    
- **Focus:** Egocentric object manipulation in mechanical assembly
    
- **Link:** https://iplab.dmi.unict.it/MECCANO
    

#### 6. **UMI Dataset**

- **By:** Stanford / Johns Hopkins / MIT
    
- **Focus:** Multimodal robot manipulation
    
- **Link:** [https://umi-data.github.io](https://umi-data.github.io)


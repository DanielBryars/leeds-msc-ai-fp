
Pi0 used segment annotations - but they didn't publish the data, but these heroes did:

## ğŸ§± Datasets on Hugging Face with Subtask Segmentation

These arenâ€™t Pi-0, but they _do_ include **temporal segment / subtask labels**:

### âœ… RoboCOIN Family

Several RoboCOIN datasets include **fine-grained subtask segmentation and labeling** alongside robot state, visuals, etc. Examples include:

- **RoboCOIN/R1_Lite_make_tea** â€“ subtask segmentation labels with scene and gripper annotations.
    
- **RoboCOIN/AIRBOT_MMK2_boxs_storage** â€“ similar subtask segment labels.
    
- **RoboCOIN/RMC-AIDA-L_pour_coffee_beans** â€“ long episodes with subtask labelling.
    

These datasets are structured (often in **LeRobot format**) with a `subtask_annotation` field per frame that indicates the current subtask ID, so you _can reconstruct temporal segments_ by grouping identical IDs over time.

ğŸ‘‰ This is conceptually very close to Pi-0â€™s segment annotation goal â€” just without natural-language labels.

---

### âœ… RealSource World (Hugging Face)

**RealSource World** is a large real-robot manipulation dataset with **fine-grained â€œatomic skillâ€ segmentation** and quality assessments for each episode.

- Multi-camera real robot data
    
- Actions + states + visual observations
    
- **Atomic skill segmentation** (akin to temporal subtask labels)
    

Although the segmentation there isnâ€™t necessarily _natural-language_, it _is_ explicit skill segmentation that can be used to train hierarchical or segmented models.

---

## ğŸ“š Research Work with Explicit Segment Labels

These arenâ€™t direct Hugging Face datasets, but they show active work in segment annotations:

### ğŸ§  **GATE-VLAP Dataset**

This is a research dataset that contains **2,124 atomic action segments** derived from manipulation trajectories with planner alignment, STRIPS-style labels, temporal bounds, and pre/postconditions.

- Each segment has start/end frames
    
- Each segment is labeled with a symbolic action + arguments
    
- Aligns low-level data with logical action semantics
    

This is closer to what Pi-0 does _conceptually_ (though smaller and symbolic in nature).

---

## ğŸ“Œ Related Trends in Segmentation

While not full trajectory segmentation datasets for robot control, these show related directions:

- Some works derive segmentation from trajectories using multimodal reasoning (e.g., vision + proprioception + LLMs) â€” e.g., **Robo2VLM** extracts sequence phases for QA tasks.
    
- Robotics foundations like **LeRobot** standardise formats with segmentation support, easing shared annotations across benchmarks.
    

---

## ğŸ§© Whatâ€™s Still _Rare_

ğŸŸ¥ **Language-conditioned segment annotations** (i.e., segments tagged with natural-language labels like â€œalign gripperâ€, â€œlift objectâ€) are still mostly **internal to proprietary datasets** like Pi-0. Public alternatives tend to:

- use **numeric subtask IDs**, or
    
- symbolic STRIPS-style labels, but
    
- rarely full phrase descriptions tied to language models.
    

---

## ğŸ“Œ Summary

|Dataset|Fine-grained segments|Language labels?|Public|
|---|---|---|---|
|RoboCOIN family|âœ”ï¸ numeric subtask IDs|âŒ|âœ”ï¸|
|RealSource World|âœ”ï¸ skills per frame|âŒ|âœ”ï¸|
|GATE-VLAP|âœ”ï¸ symbolic action segments|âœ´ï¸ symbolic (planner actions)|âœ”ï¸/paper|
|Pi-0 dataset|âœ”ï¸ language segments|âœ”ï¸ natural language|âŒ published|

---

If you want, I can point you to **code snippets** showing how to load one of these Hugging Face datasets (e.g., RoboCOIN) and extract the temporal segments â€” just ask!